ðŸ“„ Product Requirements Document (PRD)

ðŸ§© Product Name: Keywords Extractor UI

# Overview

This document outlines the requirements for the Keywords Extractor UI, a Streamlit-based application designed for internal use. The primary problem this tool solves is the time-consuming and often inconsistent manual process of gathering and enriching company data for targeted cold outreach campaigns. It provides internal analysts and sales team members with a user-friendly interface to execute a complex, multi-phase data extraction pipeline, currently orchestrated by the master_pipeline.py script. The value lies in significantly accelerating data gathering, ensuring consistency through configurable automation, and providing detailed logs and intermediate artifacts for transparency and troubleshooting, mirroring the capabilities and outputs of the underlying script.

ðŸ‘¥ Target Users

    Internal Analysts: Require detailed data, intermediate outputs (like those generated in subdirectories by the script), and configuration options for refining extraction processes and performing in-depth company analysis.
    Internal Sales Team Members: Need a fast, reliable way to generate enriched contact lists based on company criteria, without needing deep technical knowledge of the underlying pipeline. They prioritize ease of use and clear final outputs (like the final CSV generated by the script).

# Core Features

    Flexible Data Input & Validation
        What it does: Allows users to initiate the pipeline using either a CSV file or by manually entering company details directly into a web form. Supports single or multiple company entries in the manual form. Performs initial validation on uploaded CSVs to ensure required columns (company name, location, url) are present, similar to the validate_csv_columns function in the script.
        Why it's important: Provides flexibility for different data sources and scales of outreach. Ensures data integrity before starting computationally expensive processes.
        How it works: Utilizes Streamlit's file_uploader for CSVs and text_input/text_area widgets for manual entry. Input validation logic checks for required columns/fields before enabling the run.

    Configurable Pipeline Execution
        What it does: Enables users to run the full data extraction pipeline or select specific phases (Phase 1: Machine Assets, Phase 2: Keyword Crawling, Phase 3: Integration) to execute, mimicking the sequential execution flow in run_pipeline. Offers an "Advanced Configuration" section where users can override default parameters for each selected phase (parameters align with those potentially loaded from a config file or defaults used by the script). Optionally allows skipping LLM provider validation checks, similar to the --skip-llm-validation flag.
        Why it's important: Allows users to tailor the process, fine-tune performance, and control costs (e.g., LLM usage) without code changes. Provides control analogous to the script's command-line arguments and config files.
        How it works: Uses Streamlit checkboxes for phase selection. A collapsible section (st.expander) reveals forms populated with default parameters for each phase, allowing user overrides. A checkbox controls the LLM validation skip. Configurations are passed to the backend pipeline runner.

    Comprehensive Output & Artifact Access
        What it does: Provides the final enriched data as a downloadable CSV file, named using a convention like final_export_{category}_{timestamp}.csv. Allows users to download intermediate files (artifacts) generated during each phase (e.g., files stored in bundesanzeiger_html, cleaned_html, domain_content, extracted_keywords directories by the script). Displays a preview of the final output data in an interactive table within the UI.
        Why it's important: Delivers the primary required output while also supporting debugging, validation, and deeper analysis by providing access to intermediate results generated by the distinct pipeline stages. The live preview offers immediate feedback.
        How it works: Streamlit download_button widgets for file downloads (final CSV and zipped artifacts per run). st.dataframe or st.data_editor displays the output table. Links to artifacts stored on the local filesystem (organized by run ID, similar to the script's run_output_dir concept) are generated.

    Real-time Job Monitoring & Logging
        What it does: Displays the status (queued, running, completed, failed) and progress of each submitted job in real-time, potentially using a progress indicator like tqdm's output seen in the script. Provides access to detailed logs generated by the pipeline for each specific run, capturing information similar to that sent to pipeline.log by the script's setup_logging function. Includes LLM validation status if run.
        Why it's important: Offers transparency into the execution process, helps users estimate completion times, and provides crucial information for troubleshooting errors, mirroring the script's logging output.
        How it works: The UI updates dynamically to show job statuses and progress. A dedicated log viewer area displays log messages streamed from the backend process associated with a selected run ID.

    Multi-Run Support & Concurrency
        What it does: Allows multiple users to submit jobs concurrently. A global view shows all active and recently completed runs. Each run is treated as an independent job with its own configuration, logs, and outputs, managed within its own logical space (akin to the timestamped run directory in the script).
        Why it's important: Enables the tool to be used effectively by multiple team members simultaneously without blocking, increasing overall team productivity.
        How it works: A backend job runner manages a queue and executes pipeline runs in parallel (using threading or asynchronous workers, similar to how the script uses asyncio for crawling). The Streamlit UI lists jobs with unique run IDs, referencing their respective status, logs, and output files stored locally.

# User Experience

    User Personas: (As defined under Target Users)
        Analyst: Values control, detail, access to intermediate data (artifacts from phase subdirectories), configuration options.
        Sales Member: Values speed, simplicity, clear final output (final CSV), ease of input.
    Key User Flows:
        Standard CSV Run: User uploads valid CSV -> Selects 'Run All Phases' -> Clicks 'Start Run' -> Monitors progress (seeing phase updates) -> Downloads final CSV when complete.
        Manual Entry & Configured Run: User enters companies -> Selects specific phases -> Expands Advanced Config -> Modifies a parameter -> Clicks 'Start Run' -> Monitors progress -> Views output table -> Downloads final CSV and potentially intermediate artifacts.
        Troubleshooting Run: User observes a failed run -> Clicks on the run -> Views the logs to identify the error (e.g., "CSV missing required columns", "Error extracting Bundesanzeiger HTML").
    UI/UX Considerations:
        Layout: Maintain Sidebar (config, phase selection) and Main Panel (input, status, output, logs).
        Clarity: Use clear labels, tooltips for config parameters, distinct sections. Clearly indicate required input columns.
        Feedback: Provide immediate feedback on actions, clear progress indicators reflecting pipeline phases, and informative status/error messages based on script logging.
        Responsiveness: Ensure UI remains responsive during backend processing.
        Simplicity: Keep the default path straightforward; hide complexity in expandable sections.

# Technical Architecture

    System Components:
        Streamlit Frontend: UI for input, configuration, monitoring, output display/download.
        Backend Job Runner: Manages queue, orchestrates concurrent runs (threading/async), calls the main pipeline logic.
        Data Extraction Pipeline Logic (based on master_pipeline.py): Modular Python code likely refactored from master_pipeline.py and its imported modules (extracting_machines, webcrawl, merge_pipeline, utils). Encapsulates logic for each phase. Includes LLM validation logic.
        File Storage: Local filesystem for inputs, artifacts, outputs, logs, organized by run ID within a main output directory structure similar to that created by the script.
    Data Models:
        Input: CSV (requires company name, location, url columns), Manual Form Data.
        Configuration: Python dictionaries/JSON mirroring script's parameters and structure.
        Intermediate Artifacts: Files corresponding to outputs of script functions (e.g., HTML files, cleaned text, JSON data, intermediate CSVs) stored in phase-specific subdirectories.
        Output: Final CSV named final_export_{category}_{timestamp}.csv.
        Job Metadata: Run ID, status, start/end time, configuration used, paths to logs/outputs.
    APIs and Integrations:
        Internal: Python calls between UI, Job Runner, and Pipeline Logic Modules.
        External: HTTP requests to Bundesanzeiger, Web crawling of company websites, API calls to LLM provider (e.g., Bedrock).
    Infrastructure Requirements:
        Python 3.x environment with necessary libraries (Streamlit, Pandas, Requests, Playwright/Scrapy, asyncio, tqdm, LLM libraries, etc.).
        Local network (LAN) deployment.
        Sufficient local disk space for logs, artifacts, outputs.
        Compute resources (CPU, RAM) for Streamlit, pipeline phases (crawling, LLM), and concurrent jobs.

# Development Roadmap

    Phase 1: Minimum Viable Product (MVP)
        Goal: Establish the end-to-end flow wrapping the script's core functionality.
        Features:
            CSV Upload input (with column validation).
            Run all pipeline phases (no selection).
            Use default parameters (no config UI).
            Backend execution of the full pipeline for a single run.
            Display basic status (Running, Completed, Failed), latest log output, and final output table.
            Download button for the final output CSV.
            Basic logging display.
    Phase 2: Core Functionality Enhancements
        Goal: Introduce flexibility and robustness mirroring script capabilities.
        Features:
            Manual data entry.
            Phase selection UI.
            Advanced Configuration UI.
            Skip LLM validation option.
            Backend support for concurrent runs.
            UI display of job queue/list.
            Download buttons for intermediate artifacts.
    Phase 3: User Experience & Future Enhancements
        Goal: Refine usability and add nice-to-have features.
        Features:
            Config file export/import (like --config-file option).
            Zip export of all artifacts for a run.
            E-mail notifications.
            Optional admin mode.
            Improved UI feedback and error handling.
            Performance optimizations.
            Refined log viewing/filtering.
            Option to cleanup intermediate files (like the commented-out cleanup_intermediate_outputs function).

# Logical Dependency Chain

    Foundation: Basic Streamlit app; Integrate/refactor pipeline logic from master_pipeline.py; Implement simplest end-to-end execution path triggered from UI.
    MVP Frontend & Core Execution: Build CSV upload + validation; Trigger backend; Basic status/log display; Output table display; Final CSV download. Achieves MVP.
    Configuration & Control: Build phase selection UI; Advanced configuration UI; Manual entry form; Skip LLM validation checkbox; Connect UI options to backend runner.
    Concurrency & Monitoring: Refactor backend for concurrency; Implement job queue UI; Add artifact downloads; Enhance logging display.
    Refinements & Future Features: Build features from Roadmap Phase 3 based on priority.

# Risks and Mitigations

    Technical Challenges:
        Risk: LLM calls unreliable/slow/costly; extraction quality varies.
            Mitigation: Implement retries/backoff; configurable LLM parameters; caching; monitoring; include LLM validation step.
        Risk: pipeline process took long time to finish; UI blocked.
            Mitigation: Implement asynchronous backend job execution (using threads/asyncio/task queue) to prevent UI blocking; Provide real-time progress updates and status indicators in the UI; Optimize pipeline steps for performance; Optionally notify users upon completion.
        Risk: Website/Bundesanzeiger structures change; crawling blocked.
            Mitigation: Robust selectors; configurable crawl parameters; respect robots.txt; error handling per script step; monitoring.
        Risk: Dependencies between pipeline sub-modules break (import errors, API changes).
            Mitigation: Good testing practices; clear documentation of module interfaces; version pinning.
    MVP Definition & Scope Creep:
        Risk: Difficulty sticking to MVP.
            Mitigation: Prioritize core end-to-end flow wrapping script functionality; Defer non-essentials.
    Resource Constraints / Performance:
        Risk: Concurrent runs overload machine; slow phases.
            Mitigation: Concurrency limits; optimize code (e.g., use asyncio effectively); resource monitoring; provide time estimates.
    User Adoption:
        Risk: UI complex; workflow mismatch.
            Mitigation: Focus on simplicity for default path; clear labels/tooltips; gather user feedback early; provide documentation.

# Appendix

    Pipeline Phase Details (based on master_pipeline.py):
        Phase 1: Extracting Machine Assets: Validate/Filter input CSV/Excel -> Get Bundesanzeiger HTML -> Clean HTML -> Extract Sachanlagen (LLM) -> Generate Report -> Merge with original data.
        Phase 2: Crawling & Scraping Keywords: Crawl domains (asyncio) -> Extract keywords (LLM) -> Fill process type -> Pluralize keywords (LLM) -> Consolidate data (JSON) -> Convert to CSV.
        Phase 3: Final Data Integration: Merge Phase 1 output with Phase 2 output -> Enrich data -> Output final CSV.